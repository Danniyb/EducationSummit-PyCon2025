---
title: "ExecExam: A Tool to Facilitate Effective Executable Examinations in Python"
date: "2025-05-15"
date-format: long
author: "Pallas-Athena Cain, Hemani Alaparthi, and Gregory Kapfhammer"
format: 
  revealjs:
    theme: default
    slide-number: false
    incremental: false
    code-fold: true
    code-tools: true
    code-link: true
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    footer: "PyCon Education Summit 2025"
    css: ./styles.css
---

## What is an Executable Examination?

::: {.fragment style="margin-top: -0.15em; font-size: 0.8em;"}

{{< iconify fa6-solid gears >}} **Goal: Assess a student's ability to program
with real tools**

  - A student writes, modifies, and runs code to solve a real problem
  - Graded via automated tests that use Pytest tests and assertions
  - Unlike static examinations an executable examination assesses:
    - Programming logic
    - Debugging ability
    - Tool use (e.g., text editor, terminal, IDE, and Git)

:::

::: {.fragment .fade style="margin-top: -0.15em; font-size: 0.8em;"}

🎯 **Like a take-home project — but precise, consistent, and scalable!**

**Reference**: Chris Bourke, Yael Erez, and Orit Hazzan. 2023. "Executable Exams: Taxonomy,
Implementation and Prospects". In Proceedings of 54th SIGCSE.

:::

## Problems with Computing Assessments

::: fragment

{{< iconify game-icons team-idea >}} **Why do we need better assessments?**

- Manual grading is slow and inconsistent
- Students often don’t know why their code fails
- Feedback is shallow or missing altogether
- Limited assessment of effective tool use
- Pytest not a good fit for assessment

:::

::: {.fragment .fade style="margin-top: -0.15em; font-size: 0.9em;"}

🚫 “Test failed” isn't enough! ExecExam presents a compelling alternative to
either manual assessment or running only Pytest.

:::

## What is ExecExam?

::: fragment

{{< iconify fa6-solid gears >}} **Scalable, feedback-rich assessment tool built in Python**

:::: {.columns}

::: {.column width="65%"}

- Runs Pytest tests on student code
- Reports all test failures and context
- Clearly explains why a test failed
- Suggests how to fix tested function
- Uses LLMs for enhanced feedback
:::

::: {.column width="35%" .middle}
![](ExecExam_-_Logo_-_300.png){width="100%"}
:::

:::

::: {.fragment .fade style="margin-top: -0.75em; font-size: 0.9em;"}

{{< iconify fa6-solid lightbulb >}} **Next Step**: Explore ExecExam's features
and how teachers can integrate them into the assessments for their programming courses!

:::

::::

## Understanding ExecExam's Output

![](terminal.png)

## Key Features of ExecExam

::: fragment

{{< iconify fa6-solid lightbulb >}} **Why use ExecExam for your next assessment?**

:::

::: incremental

- 🧪 Configured Pytest runs for streamlined assessment
- 💻 Runs on student laptop through assessment process
- 📜 Provides contextualized, detailed test failure reports
- ⚙️ Integrates with GitHub and GitHub Actions for CI/CD
- 🧠 Features flexible, democratized LLM-powered debugging
- 🔁 Offers actionable insights to instructors and students!
- 🛠️ Open-source tool collaboratively developed on GitHub

:::

## For Instructors

**How to adopt it**

- Design scaffolded coding tasks
- Write tests
- Deploy with GitHub Actions or CLI
- Grade fairly, at scale

🧰 Lightweight setup—just Python, Git, and your test cases

## Future Work

::: {.fragment style="margin-top: -0.15em; font-size: 0.7em;"}

**What’s next for ExecExam?**

- 📊 Analytics & Instructor Features
  - Store test outcomes and feedback over time
  - Visualize student debugging and improvement paths
  - Log LLM interactions to evaluate effectiveness
  - Hold out hidden test cases for instructor-only grading

- 🧠 Adaptive Feedback Loops
  - Tailor feedback complexity to student performance
  - Allow students to rate LLM feedback

:::

## Key Takeaways

::: {.fragment .fade .tight-boxed-content style="margin-top: -0.15em; font-size: 0.7em;"}

**Better exam grading, better learning**

- Helps students debug and learn
- Saves teachers time
- More fair + consistent grading
- Scales to large classes

:::

::: {.fragment .fade .tight-boxed-content style="margin-top: -0.15em; font-size: 0.6em;"}

**🚀 Try ExecExam! Let’s build smarter CS assessments!**

- 🔗 GitHub Repository: [https://github.com/GatorEducator/execexam](https://github.com/GatorEducator/execexam)
- 💻 PyPI: [https://pypi.org/project/execexam/](https://pypi.org/project/execexam/)
- 🤝 Reach out or contribute 
  - Pallas-Athena Cain: **cain01@allegheny.edu** 
  - Gregory M. Kapfhammer: **gkapfham@allegheny.edu**

🦚 Consider a birds of a feather session about automated grading

:::

## References

::: {.fragment .tight-boxed-content style="margin-top: -0.15em; font-size: 0.5em;"}

- João Paulo Barros, Luís Estevens, Rui Dias, Rui Pais, and Elisabete Soeiro.
2003. Using lab exams to ensure programming practice in an introductory
programming course. SIGCSE Bull. 35, 3 (September 2003), 16–20.
- Jonathan Corley, Ana Stanescu, Lewis Baumstark, and Michael C. Orsega. 2020.
Paper Or IDE? The Impact of Exam Format on Student Performance in a CS1 Course.
In Proceedings of the 51st ACM Technical Symposium on Computer Science
Education (SIGCSE '20). Association for Computing Machinery, New York, NY, USA,
706–712.
- Scott Grissom, Laurie Murphy, Renée McCauley, and Sue Fitzgerald. 2016. Paper
vs. Computer-based Exams: A Study of Errors in Recursive Binary Tree
Algorithms. In Proceedings of the 47th ACM Technical Symposium on Computing
Science Education (SIGCSE '16). Association for Computing Machinery, New York,
NY, USA, 6–11.
- Sevkli, Z. 2024. Assessing the Impact of Open-Resource Access on Student
Performance in Computer-Based Examinations. In Proceedings of the 2024 ASEE
Annual Conference & Exposition, Portland, Oregon.
- Vesa Lappalainen, Antti-Jussi Lakanen, and Harri Högmander. 2016. Paper-based
vs computer-based exams in CS1. In Proceedings of the 16th Koli Calling
International Conference on Computing Education Research (Koli Calling '16).
Association for Computing Machinery, New York, NY, USA, 172–173.

:::
