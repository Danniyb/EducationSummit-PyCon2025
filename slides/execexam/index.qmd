---
title: "ExecExam: A Tool to Facilitate Effective Executable Examinations in Python"
date: "2025-05-15"
date-format: long
author: Pallas-Athena Cain, Hemani Alaparthi, and Gregory Kapfhammer
format: 
  revealjs:
    theme: default
    slide-number: false
    incremental: false
    code-fold: true
    code-tools: true
    code-link: true
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    footer: "PyCon Education Summit 2025"
    css: ./styles.css
---

## What is an Executable Examination?

::: {.fragment style="margin-top: -0.15em; font-size: 0.8em;"}

**Goal: Assess a student's ability to program with real tools**

  - A student writes, modifies, and runs code to solve a real problem
  - Graded via automated tests that use Pytest tests and assertions
  - Unlike static examinations an executable examination assesses:
    - Programming logic
    - Debugging ability
    - Tool use (editor, terminal, Git)

🎯 Like a take-home project — but precise, consistent, and scalable

:::

## The Problem

**Why do we need better assessments?**

- Manual grading is slow and inconsistent
- Students often don’t know why their code fails
- Feedback is shallow or missing altogether

🚫 “Test failed” isn’t enough.

## What is ExecExam?

**Scalable, feedback-rich assessment tool**

- Runs Pytest on student code
- Unlike Pytest, continues past the first failure
- Explains why a test failed
- Suggests how to fix it
- Utilizes LLMs for even better feedback

![](ExecExam_-_Logo_-_300.png)

## 

![](terminal.png)

## Key Features

**Why use ExecExam?**

- 🧪 Full Pytest coverage, Streamlines assessment
- 💻 Can be run throughout the student coding process
- 🧠 AI-powered failure analysis
- ⚙️ GitHub integration, easy CI/CD
- 🔁 Enhance the learning experience by offering actionable insights throughout the coding process

## For Instructors

**How to adopt it**

- Design scaffolded coding tasks
- Write tests
- Deploy with GitHub Actions or CLI
- Grade fairly, at scale

🧰 Lightweight setup—just Python, Git, and your test cases

## Future Work

::: {.fragment style="margin-top: -0.15em; font-size: 0.7em;"}

**What’s next for ExecExam?**

- 📊 Analytics & Instructor Features
  - Store test outcomes and feedback over time
  - Visualize student debugging and improvement paths
  - Log LLM interactions to evaluate effectiveness
  - Hold out hidden test cases for instructor-only grading

- 🧠 Adaptive Feedback Loops
  - Tailor feedback complexity to student performance
  - Allow students to rate LLM feedback

:::

## Key Takeaways

::: {.fragment .fade .tight-boxed-content style="margin-top: -0.15em; font-size: 0.7em;"}

**Better exam grading, better learning**

- Helps students debug and learn
- Saves teachers time
- More fair + consistent grading
- Scales to large classes

:::

::: {.fragment .fade .tight-boxed-content style="margin-top: -0.15em; font-size: 0.6em;"}

**🚀 Try ExecExam! Let’s build smarter CS assessments!**

- 🔗 GitHub Repository: [https://github.com/GatorEducator/execexam](https://github.com/GatorEducator/execexam)
- 💻 PyPI: [https://pypi.org/project/execexam/](https://pypi.org/project/execexam/)
- 🤝 Reach out or contribute 
  - Pallas-Athena Cain: **cain01@allegheny.edu** 
  - Gregory M. Kapfhammer: **gkapfham@allegheny.edu**

🦚 Consider a birds of a feather session about automated grading

:::

## References

::: {.fragment .tight-boxed-content style="margin-top: -0.15em; font-size: 0.5em;"}

- João Paulo Barros, Luís Estevens, Rui Dias, Rui Pais, and Elisabete Soeiro.
2003. Using lab exams to ensure programming practice in an introductory
programming course. SIGCSE Bull. 35, 3 (September 2003), 16–20.
https://doi.org/10.1145/961290.961519
- Jonathan Corley, Ana Stanescu, Lewis Baumstark, and Michael C. Orsega. 2020.
Paper Or IDE? The Impact of Exam Format on Student Performance in a CS1 Course.
In Proceedings of the 51st ACM Technical Symposium on Computer Science
Education (SIGCSE '20). Association for Computing Machinery, New York, NY, USA,
706–712. https://doi.org/10.1145/3328778.3366857
- Scott Grissom, Laurie Murphy, Renée McCauley, and Sue Fitzgerald. 2016. Paper
vs. Computer-based Exams: A Study of Errors in Recursive Binary Tree
Algorithms. In Proceedings of the 47th ACM Technical Symposium on Computing
Science Education (SIGCSE '16). Association for Computing Machinery, New York,
NY, USA, 6–11. https://doi.org/10.1145/2839509.2844587
- Sevkli, Z. 2024. Assessing the Impact of Open-Resource Access on Student
Performance in Computer-Based Examinations. In Proceedings of the 2024 ASEE
Annual Conference & Exposition, Portland, Oregon.
https://doi.org/10.18260/1-2--46619
- Vesa Lappalainen, Antti-Jussi Lakanen, and Harri Högmander. 2016. Paper-based
vs computer-based exams in CS1. In Proceedings of the 16th Koli Calling
International Conference on Computing Education Research (Koli Calling '16).
Association for Computing Machinery, New York, NY, USA, 172–173.
https://doi.org/10.1145/2999541.2999565

:::
