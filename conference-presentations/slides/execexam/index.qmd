---
title: "ExecExam: A Tool to Facilitate Effective Executable Examinations in Python"
subtitle: Pallas-Athena Cain and Gregory Kapfhammer
format: 
  revealjs:
    theme: default
    slide-number: false
    incremental: true
    code-fold: true
    code-tools: true
    code-link: true
    history: false
    scrollable: true
    transition: slide
    highlight-style: github
    footer: "PyCon Education Summit 2025"
    css: ../../css/styles.css
---


## Abstract

An executable examinations invites a learner to write, modify, and/or execute Python source code to solve well-specified programming problems. This type of assessment enables teachers to determine whether or not a learner can effectively use a suite of software development tools to implement a program that meets a specification. After highlighting the challenges associated with delivering and assessing executable examinations at scale, this presentation introduces ExecExam, a tool that streamlines and automates the assessment of Python programs. Leveraging an integration with both local and cloud-based LLMs, ExecExam provides sophisticated, context-aware feedback for failing Pytest tests, guiding students towards the effective resolution of errors while fostering a deeper understanding of Python programming concepts. Instead of receiving output from a single failed test assertion, students using ExecExam see a detailed report of all failing checks and specific suggestions for code fixes or alternative approaches that will enable the student's project to pass the tests. In addition to overviewing the design and implementation of ExecExam, this talk will equip teachers with the knowledge needed to deploy their own executable examinations with the ExecExam tool. More details about the ExecExam tool is available in its GitHub repository at [https://github.com/GatorEducator/execexam](https://github.com/GatorEducator/execexam).

## Key Takeaways

- Learn the benefits of executable examinations for assessing programming skill
- Master the use of testing frameworks to verify student code submissions
- Design scaffolded programming challenges with appropriate difficulty levels
- Deploy context-aware feedback systems that help students learn from mistakes
- Integrate automated assessment tools into your existing teaching workflow
- Create a more equitable evaluation process through consistent automated grading
- Learn how to use local and cloud-based LLMs to provide context-aware feedback

## Introduction: What is an Execuatable Examination?

**Assessing real programming ability**

  - A student writes, modifies, and runs code to solve a real problem
  - Graded via automated tests (e.g. Pytest)
  - Unlike static exams, this tests:
    - Programming logic
    - Debugging ability
    - Tool use (editor, terminal, Git)

üéØ It‚Äôs like a take-home project ‚Äî but precise, consistent, and scalable

## The Problem

**Why do we need better assessments?**

- Manual grading is slow and inconsistent
- Students often don‚Äôt know why their code fails
- Feedback is shallow or missing altogether

üö´ ‚ÄúTest failed‚Äù isn‚Äôt enough.

## What is ExecExam?

**Scalable, feedback-rich assessment**

- Runs Pytest on student code
- Explains why a test failed
- Suggests how to fix it
- Utilizes LLMs for even better feedback

![](ExecExam_-_Logo_-_300.png)

## Key Features

**Why use ExecExam?**

- üß™ Full Pytest coverage, Streamlines assessment
- üß† AI-powered failure analysis
- ‚öôÔ∏è GitHub integration, easy CI/CD
- üîÅ Enhance the learning experience by offering actionable insights throughout the coding process

## For Instructors

**How to adopt it**

- Design scaffolded coding tasks
- Write tests
- Deploy with GitHub Actions or CLI
- Grade fairly, at scale

üß∞ Lightweight setup‚Äîjust Python, Git, and your test cases

## Future Work

**What‚Äôs next for ExecExam?**

- üß™ Progress Tracking & Learning Analytics
  - Store test outcomes and feedback over time
  - Visualize student debugging and improvement paths
  - Enable instructors to spot misconceptions early

- üß† Adaptive Feedback Loops
  - Tailor feedback complexity to student performance
  - Suggest additional scaffolded tasks based on error patterns

- üìä Analytics & Instructor Features
  - Build tools for tracking student progress over time
  - Log LLM interactions to evaluate effectiveness
  - Hold out hidden test cases for instructor-only grading

## Takeaways

**Better exam grading, better learning**

- Helps students debug and learn
- Saves teachers time
- More fair + consistent grading
- Scales to large classes

## Try ExecExam

- üîó GitHub Repository: [https://github.com/GatorEducator/execexam](https://github.com/GatorEducator/execexam)
- ü§ù Reach out or contribute
- üöÄ Let‚Äôs build smarter CS assessments

## References

- Jo√£o Paulo Barros, Lu√≠s Estevens, Rui Dias, Rui Pais, and Elisabete Soeiro. 2003. Using lab exams to ensure programming practice in an introductory programming course. SIGCSE Bull. 35, 3 (September 2003), 16‚Äì20. https://doi.org/10.1145/961290.961519
- Jonathan Corley, Ana Stanescu, Lewis Baumstark, and Michael C. Orsega. 2020. Paper Or IDE? The Impact of Exam Format on Student Performance in a CS1 Course. In Proceedings of the 51st ACM Technical Symposium on Computer Science Education (SIGCSE '20). Association for Computing Machinery, New York, NY, USA, 706‚Äì712. https://doi.org/10.1145/3328778.3366857
- Scott Grissom, Laurie Murphy, Ren√©e McCauley, and Sue Fitzgerald. 2016. Paper vs. Computer-based Exams: A Study of Errors in Recursive Binary Tree Algorithms. In Proceedings of the 47th ACM Technical Symposium on Computing Science Education (SIGCSE '16). Association for Computing Machinery, New York, NY, USA, 6‚Äì11. https://doi.org/10.1145/2839509.2844587
- Sevkli, Z. 2024. Assessing the Impact of Open-Resource Access on Student Performance in Computer-Based Examinations. In Proceedings of the 2024 ASEE Annual Conference & Exposition, Portland, Oregon. https://doi.org/10.18260/1-2--46619
- Vesa Lappalainen, Antti-Jussi Lakanen, and Harri H√∂gmander. 2016. Paper-based vs computer-based exams in CS1. In Proceedings of the 16th Koli Calling International Conference on Computing Education Research (Koli Calling '16). Association for Computing Machinery, New York, NY, USA, 172‚Äì173. https://doi.org/10.1145/2999541.2999565

## Thank you!

We would love to continue the conversation! Please find us after the session if you have questions or want to discuss implementing this approach in your courses.

Repository: [https://github.com/GatorEducator/execexam ](https://github.com/GatorEducator/execexam )

PyPI: https://pypi.org/project/execexam/

Pallas-Athena Cain: **cain01@allegheny.edu**

Gregory M. Kapfhammer: **gkapfhammer@allegheny.edu**