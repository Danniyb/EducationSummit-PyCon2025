---
title: "ExecExam: A Tool to Facilitate Effective Executable Examinations in Python"
subtitle: Pallas-Athena Cain, Gregory Kapfhammer
format: 
  revealjs:
    theme: default
    slide-number: true
    incremental: true
    code-fold: true
    code-tools: true
    code-link: true
    highlight-style: github
    footer: "Allegheny College"
    css: styles.css
---

## Abstract

An executable examinations invites a learner to write, modify, and/or execute Python source code to solve well-specified programming problems. This type of assessment enables teachers to determine whether or not a learner can effectively use a suite of software development tools to implement a program that meets a specification. After highlighting the challenges associated with delivering and assessing executable examinations at scale, this presentation introduces ExecExam, a tool that streamlines and automates the assessment of Python programs. Leveraging an integration with both local and cloud-based LLMs, ExecExam provides sophisticated, context-aware feedback for failing Pytest tests, guiding students towards the effective resolution of errors while fostering a deeper understanding of Python programming concepts. Instead of receiving output from a single failed test assertion, students using ExecExam see a detailed report of all failing checks and specific suggestions for code fixes or alternative approaches that will enable the student's project to pass the tests. In addition to overviewing the design and implementation of ExecExam, this talk will equip teachers with the knowledge needed to deploy their own executable examinations with the ExecExam tool. More details about the ExecExam tool is available in its GitHub repository at [https://github.com/GatorEducator/execexam](https://github.com/GatorEducator/execexam).

## Key Takeaways

- Learn the benefits of executable examinations for assessing programming skill
- Master the use of testing frameworks to verify student code submissions
- Design scaffolded programming challenges with appropriate difficulty levels
- Deploy context-aware feedback systems that help students learn from mistakes
- Integrate automated assessment tools into your existing teaching workflow
- Create a more equitable evaluation process through consistent automated grading
- Learn how to use local and cloud-based LLMs to provide context-aware feedback

## Introduction: What is an Execuatable Examination?

**Assessing real programming ability**

  - A student writes, modifies, and runs code to solve a real problem
  - Graded via automated tests (e.g. Pytest)
  - Unlike static exams, this tests:
    - Programming logic
    - Debugging ability
    - Tool use (editor, terminal, Git)

🎯 It’s like a take-home project — but precise, consistent, and scalable

## The Problem

**Why do we need better assessments?**

- Manual grading is slow and inconsistent
- Students often don’t know why their code fails
- Feedback is shallow or missing altogether

🚫 “Test failed” isn’t enough.

## What is ExecExam?

**Scalable, feedback-rich assessment**

- Runs Pytest on student code
- Explains why a test failed
- Suggests how to fix it
- Utilizes LLMs for even better feedback

## Key Features

**Why use ExecExam?**

- 🧪 Full Pytest coverage, Streamlines assessment
- 🧠 AI-powered failure analysis
- ⚙️ GitHub integration, easy CI/CD
- 💻 Local & cloud LLM support
- 🔁 Enhance the learning experience by offering actionable insights throughout the coding process

## For Instructors

**How to adopt it**

- Design scaffolded coding tasks
- Write tests
- Deploy with GitHub Actions or CLI
- Grade fairly, at scale

🧰 Lightweight setup—just Python, Git, and your test cases

## Future Work

**What’s next for ExecExam?**

🧪 Progress Tracking & Learning Analytics
   - Store test outcomes and feedback over time
   - Visualize student debugging and improvement paths
   - Enable instructors to spot misconceptions early

🧠 Adaptive Feedback Loops
   - Tailor feedback complexity to student performance
   - Suggest additional scaffolded tasks based on error patterns

📊 Analytics & Instructor Features
   - Build tools for tracking student progress over time
   - Log LLM interactions to evaluate effectiveness
   - Hold out hidden test cases for instructor-only grading

## Takeaways

**Better exam grading, better learning**

- Helps students debug and learn
- Saves teachers time
- More fair + consistent grading
- Scales to large classes

## Try ExecExam

- 🔗 GitHub Repository: [https://github.com/GatorEducator/execexam](https://github.com/GatorEducator/execexam)
- 🤝 Reach out or contribute
- 🚀 Let’s build smarter CS assessments
